DATA:
  data_name: cuboid
  data_root: [dataset/dcf_cuboid, dataset/dcf_polycube, dataset/dcf_scannet] # Fill in the pre-processed data path (which contains the 'train', 'val', 'test' directories)
  classes: 19
  fea_dim: 6
  voxel_size: 0.02   # todo
  voxel_max: 120000  # maximum number of input samples
  loop: 6 

TRAIN:
  # arch
  name: voxel0.02_offsetw1_tanh_epoch200 # experiment name todo 
  activation: Tanh
  # gahye todo
  edge_weight: 1.5  # edge weight for crossentropy
  concave_weight: 5  # edge weight for crossentropy
  offset_weight: 1  # weight for offset loss term
  train_gpu: [7]  # todo [0, 1, 2, 3]
  train_continue: False
  coord_move: True
  
  arch: stratified_transformer
  stem_transformer: False
  use_xyz: True
  sync_bn: True  # adopt sync_bn or not
  rel_query: True
  rel_key: True
  rel_value: True
  quant_size: 0.005
  downsample_scale: 4 
  num_layers: 5 
  patch_size: 1 
  window_size: 5 
  depths: [3,3,9,3,3] 
  channels: [48, 96, 192, 384, 384] 
  num_heads: [3, 6, 12, 24, 24]
  up_k: 3
  drop_path_rate: 0.3
  concat_xyz: True
  grid_size: 0.02
  max_batch_points: 600000 # 250000
  max_num_neighbors: 34 # For KPConv
  ratio: 0.25
  k: 16

  # training
  aug: False  # random transformation
  transformer_lr_scale: 0.1 
  scheduler_update: step 
  scheduler: MultiStepWithWarmup 
  warmup: linear
  warmup_iters: 3000
  warmup_ratio: 0.000001
  use_amp: True
  optimizer: AdamW #SGD
  workers: 0  # data loader workers
  batch_size: 20 # batch size for training
  batch_size_val: 10 # batch size for validation during training, memory and speed tradeoff
  viz_size_val: 1
  base_lr: 0.006
  epochs: 100
  start_epoch: 0
  step_epoch: 30
  multiplier: 0.1
  momentum: 0.9
  weight_decay: 0.05
  drop_rate: 0.5

  ignore_label: -100 #255
  manual_seed: 123
  print_freq: 1
  save_freq: 9
  save_path: runs #dcf_stratified_transformer  # corresponding to name
  result_path: results #dcf_stratified_transformer  # corresponding to name
  weight: checkpoints # path to initial weight (default: none)
  resume:  # path to latest checkpoint (default: none)
  evaluate: True  # evaluate on validation set, extra gpu memory needed and small batch_size_val is recommend
  eval_freq: 1
Distributed:
  dist_url: tcp://127.0.0.1:6789
  dist_backend: 'nccl'
  multiprocessing_distributed: False
  world_size: 1
  rank: 0

TEST:
  eval_data_path: eval_repo/eval_scannet_filter  # test data path (no label)
  data_root_val: data/dcf/test # Fill the path that contains the scenes of the validation set (e.g., "[YOUR PATH]/val")
  split: val  # split in [train, val and test]
  test_gpu: [4] # todo 
  test_workers: 0
  batch_size_test: 1
  model_path: model_last.pth # Fill the path of the trained .pth file model
  eval_folder: eval_scannet_filter # todo 
  names_path: data/scannet/scannet_names.txt
